{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "\n",
    "# Predicting Shots Made Per Game by Kobe Bryant\n",
    "\n",
    "_Authors: Kiefer Katovich (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "In this lab you'll be using regularized regression penalties — ridge, lasso, and elastic net — to try and predict how many shots Kobe Bryant made per game during his career.\n",
    "\n",
    "The Kobe Shots data set contains hundreds of columns representing different characteristics of each basketball game. Fitting an ordinary linear regression using every predictor would dramatically overfit the model, considering the limited number of observations (games) we have available. Plus, many of the predictors have significant multicollinearity. \n",
    "\n",
    "\n",
    "**Warning:** Some of these calculations are computationally expensive and may take a while to execute. It may be worthwhile to only use a portion of the data to perform these calculations, especially if you've experienced kernel issues in the past.\n",
    "\n",
    "---\n",
    "\n",
    "### 1) Load packages and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import patsy\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kobe = pd.read_csv('./datasets/kobe_superwide_games.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2) Examine the data.\n",
    "\n",
    "- How many columns are there?\n",
    "- Examine what the observations (rows) and columns represent.\n",
    "- Why might regularization be particularly useful for modeling this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3) Create predictor and target variables. Standardize the predictors.\n",
    "\n",
    "Why is normalization necessary for regularized regressions?\n",
    "\n",
    "Use the `sklearn.preprocessing` class `StandardScaler` to standardize the predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. Build a linear regression predicting `SHOTS_MADE` from the rest of the columns.\n",
    "\n",
    "Cross-validate the $R^2$ of an ordinary linear regression model with 10 cross-validation folds.\n",
    "\n",
    "How does it perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 5) Find an optimal value for the ridge regression alpha using `RidgeCV`.\n",
    "\n",
    "Go to the documentation and [read how RidgeCV works](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html).\n",
    "\n",
    "> *Hint: Once the RidgeCV is fit, the attribute `.alpha_` contains the best alpha parameter it found through cross-validation.*\n",
    "\n",
    "Recall that ridge performs best when searching alphas through logarithmic space (`np.logspace`). This may take awhile to fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 6) Cross-validate the ridge regression $R^2$ with the optimal alpha.\n",
    "\n",
    "Is it better than the linear regression? If so, why might this be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 7) Find an optimal value for lasso regression alpha using `LassoCV`.\n",
    "\n",
    "Go to the documentation and [read how LassoCV works](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html). It is very similar to `RidgeCV`.\n",
    "\n",
    "> *Hint: Again, once the `LassoCV` is fit, the attribute `.alpha_` contains the best alpha parameter it found through cross-validation.*\n",
    "\n",
    "Recall that lasso, unlike ridge, performs best when searching for alpha through linear space (`np.linspace`). However, you can actually let the LassoCV decide what alphas to use itself by setting the keyword argument `n_alphas=` to however many alphas you want it to search over. We recommend letting scikit-learn choose the range of alphas.\n",
    "\n",
    "_**Tip:** If you find your CV taking a long time and you're not sure if it's working, set `verbose =1`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 8) Cross-validate the lasso $R^2$ with the optimal alpha.\n",
    "\n",
    "Is it better than the linear regression? Is it better than ridge? What do the differences in results imply about the issues with the data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 9) Look at the coefficients for variables in the lasso.\n",
    "\n",
    "1. Show the coefficient for variables, ordered from largest to smallest coefficient by absolute value.\n",
    "2. What percent of the variables in the original data set are \"zeroed-out\" by the lasso?\n",
    "3. What are the most important predictors for how many shots Kobe made in a game?\n",
    "\n",
    "> **Note:** If you only fit the lasso within `cross_val_score`, you'll have to refit it outside of that function to pull out the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 10) Find an optimal value for elastic net regression alpha using `ElasticNetCV`.\n",
    "\n",
    "Go to the documentation and [read how ElasticNetCV works](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNetCV.html).\n",
    "\n",
    "Note that here you'll be optimizing both the alpha parameter and the l1_ratio:\n",
    "- `alpha`: Strength of regularization.\n",
    "- `l1_ratio`: Amount of ridge vs. lasso (0 = all ridge, 1 = all lasso).\n",
    "    \n",
    "Do not include 0 in the search for `l1_ratio` — it won't allow it and will break.\n",
    "\n",
    "You can use `n_alphas` for the alpha parameters instead of setting your own values, which we highly recommend.\n",
    "\n",
    "Also, be careful setting too many l1_ratios over cross-validation folds in your search. It can take a long time if you choose too many combinations and, for the most part, there are diminishing returns in this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 11) Cross-validate the elastic net $R^2$ with the optimal alpha and l1_ratio.\n",
    "\n",
    "How does it compare to the ridge and lasso regularized regressions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 12. [Bonus] Compare the residuals for ridge and lasso visually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A: Maybe a jointplot?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
